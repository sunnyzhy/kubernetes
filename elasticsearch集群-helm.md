# elasticsearch 集群 - helm

## 前言

- 准备三台物理机
    |物理机IP|物理机HostName|角色|
    |--|--|--|
    |192.168.5.163|centos-docker-163|manager|
    |192.168.5.164|centos-docker-164|worker|
    |192.168.5.165|centos-docker-165|worker|

- nfs 根目录: ```/nfs/data```

- 集群相关资料:

    1. Elasticsearch
        ```
        https://github.com/elastic/helm-charts/tree/main/elasticsearch
        
        https://docs.bitnami.com/kubernetes/apps/elasticsearch/get-started/
        ```
 
    2. Kibana
        ```
        https://github.com/elastic/helm-charts/tree/main/kibana
        
        https://docs.bitnami.com/kubernetes/apps/kibana/get-started/
        ```

    3. Kibana 连接 Elasticsearch(带安全验证)
        ```
        https://github.com/bitnami/charts/issues/10076
        ```
## 创建 elasticsearch 目录

```bash
# mkdir -p /usr/local/k8s/elasticsearch
```

## 查找 chart

```bash
# helm search repo elasticsearch
NAME                         	CHART VERSION	APP VERSION	DESCRIPTION                                       
aliyun/elasticsearch-exporter	0.1.2        	1.0.2      	Elasticsearch stats exporter for Prometheus       
bitnami/elasticsearch        	19.1.4       	8.3.3      	Elasticsearch is a distributed search and analy...
aliyun/elastalert            	0.1.1        	0.1.21     	ElastAlert is a simple framework for alerting o...
aliyun/kibana                	0.2.2        	6.0.0      	Kibana is an open source data visualization plu...
bitnami/dataplatform-bp2     	12.0.5       	1.0.1      	DEPRECATED This Helm chart can be used for the ...
bitnami/grafana              	7.6.5        	8.3.4      	Grafana is an open source, feature rich metrics...
bitnami/kibana               	10.1.17      	8.3.3      	Kibana is an open source, browser based analyti...
```

## 拉取 chart

```bash
# helm pull bitnami/elasticsearch -d /usr/local/k8s/elasticsearch
```

## 修改 chart 配置

```bash
# tar zxvf /usr/local/k8s/elasticsearch/elasticsearch-19.1.4.tgz -C /usr/local/k8s/elasticsearch

# vim /usr/local/k8s/elasticsearch/elasticsearch/values.yaml
```

```yml
global:
  storageClass: "nfs-client"
  kibanaEnabled: true

extraEnvVars:
  - name: TZ
    value: Asia/Shanghai

security:
  enabled: true
  elasticPassword: "root"
  tls:
    autoGenerated: true

master:
  replicaCount: 3
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"

data:
  replicaCount: 3
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"

coordinating:
  replicaCount: 3
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"

ingest:
  replicaCount: 3
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
```

如果 elasticsearch 开启了 ```X-Pack(security.enabled=true)```，就需要配置 kibana ```./charts/kibana/values.yaml```:

```bash
# vim /usr/local/k8s/elasticsearch/elasticsearch/charts/kibana/values.yaml
```

```yml
extraEnvVars:
  - name: TZ
    value: Asia/Shanghai

# tls:
#   enabled: true
#   autoGenerated: true

elasticsearch:
  security:
    auth:
      enabled: true
      kibanaPassword: "root"
      createSystemUser: true
      elasticsearchPasswordSecret: "elasticsearch-cluster"
    tls:
      enabled: true
      existingSecret: "elasticsearch-cluster-coordinating-crt"
      usePemCerts: true
```

注:

1. elasticsearch chart 自带 kibana chart，只需配置 ```global.kibanaEnabled=true``` 即可开启 kibana 服务
2. 如果 elasticsearch 开启了 ```X-Pack``` ，则:
   - pod 启动时间会比较长
   - 务必修改 ```./charts/kibana/values.yaml``` 里 ```elasticsearch.security``` 的相关配置（以下只列出了需要注意的几点配置）:
      - ```elasticsearch.security.tls.existingSecret: <release-name>-coordinating-crt```
      - ```elasticsearch.security.auth.elasticsearchPasswordSecret: <release-name>```，值必须是包含 ```key=elasticsearch-password'``` 的 secret
      - ```tls.enabled=true```, 使用 ```https``` 访问 kibana; ```tls.enabled=false```, 使用 ```http``` 访问 kibana

## 重新制作 chart

```bash
# rm -rf /usr/local/k8s/elasticsearch/elasticsearch-19.1.4.tgz

# helm package /usr/local/k8s/elasticsearch/elasticsearch -d /usr/local/k8s/elasticsearch

# tree /usr/local/k8s/elasticsearch
/usr/local/k8s/elasticsearch
├── elasticsearch
│   ├── Chart.lock
│   ├── charts
│   │   ├── common
│   │   │   ├── Chart.yaml
│   │   │   ├── README.md
│   │   │   ├── templates
│   │   │   │   ├── _affinities.tpl
│   │   │   │   ├── _capabilities.tpl
│   │   │   │   ├── _errors.tpl
│   │   │   │   ├── _images.tpl
│   │   │   │   ├── _ingress.tpl
│   │   │   │   ├── _labels.tpl
│   │   │   │   ├── _names.tpl
│   │   │   │   ├── _secrets.tpl
│   │   │   │   ├── _storage.tpl
│   │   │   │   ├── _tplvalues.tpl
│   │   │   │   ├── _utils.tpl
│   │   │   │   ├── validations
│   │   │   │   │   ├── _cassandra.tpl
│   │   │   │   │   ├── _mariadb.tpl
│   │   │   │   │   ├── _mongodb.tpl
│   │   │   │   │   ├── _mysql.tpl
│   │   │   │   │   ├── _postgresql.tpl
│   │   │   │   │   ├── _redis.tpl
│   │   │   │   │   └── _validations.tpl
│   │   │   │   └── _warnings.tpl
│   │   │   └── values.yaml
│   │   └── kibana
│   │       ├── Chart.lock
│   │       ├── charts
│   │       │   └── common
│   │       │       ├── Chart.yaml
│   │       │       ├── README.md
│   │       │       ├── templates
│   │       │       │   ├── _affinities.tpl
│   │       │       │   ├── _capabilities.tpl
│   │       │       │   ├── _errors.tpl
│   │       │       │   ├── _images.tpl
│   │       │       │   ├── _ingress.tpl
│   │       │       │   ├── _labels.tpl
│   │       │       │   ├── _names.tpl
│   │       │       │   ├── _secrets.tpl
│   │       │       │   ├── _storage.tpl
│   │       │       │   ├── _tplvalues.tpl
│   │       │       │   ├── _utils.tpl
│   │       │       │   ├── validations
│   │       │       │   │   ├── _cassandra.tpl
│   │       │       │   │   ├── _mariadb.tpl
│   │       │       │   │   ├── _mongodb.tpl
│   │       │       │   │   ├── _mysql.tpl
│   │       │       │   │   ├── _postgresql.tpl
│   │       │       │   │   ├── _redis.tpl
│   │       │       │   │   └── _validations.tpl
│   │       │       │   └── _warnings.tpl
│   │       │       └── values.yaml
│   │       ├── Chart.yaml
│   │       ├── README.md
│   │       ├── templates
│   │       │   ├── configmap.yaml
│   │       │   ├── deployment.yaml
│   │       │   ├── extra-list.yaml
│   │       │   ├── _helpers.tpl
│   │       │   ├── ingress.yaml
│   │       │   ├── NOTES.txt
│   │       │   ├── plugins-configmap.yaml
│   │       │   ├── pvc.yaml
│   │       │   ├── saved-objects-configmap.yaml
│   │       │   ├── secret.yaml
│   │       │   ├── serviceaccount.yaml
│   │       │   ├── servicemonitor.yaml
│   │       │   ├── service.yaml
│   │       │   └── tls-secret.yaml
│   │       └── values.yaml
│   ├── Chart.yaml
│   ├── README.md
│   ├── templates
│   │   ├── configmap.yaml
│   │   ├── coordinating
│   │   │   ├── hpa.yaml
│   │   │   ├── serviceaccount.yaml
│   │   │   ├── statefulset.yaml
│   │   │   └── svc-headless.yaml
│   │   ├── data
│   │   │   ├── hpa.yaml
│   │   │   ├── serviceaccount.yaml
│   │   │   ├── statefulset.yaml
│   │   │   └── svc-headless.yaml
│   │   ├── extra-list.yaml
│   │   ├── _helpers.tpl
│   │   ├── ingest
│   │   │   ├── hpa.yaml
│   │   │   ├── ingress.yaml
│   │   │   ├── serviceaccount.yaml
│   │   │   ├── service.yaml
│   │   │   ├── statefulset.yaml
│   │   │   └── svc-headless.yaml
│   │   ├── ingress-tls-secrets.yaml
│   │   ├── ingress.yaml
│   │   ├── initialization-configmap.yaml
│   │   ├── master
│   │   │   ├── hpa.yaml
│   │   │   ├── serviceaccount.yaml
│   │   │   ├── statefulset.yaml
│   │   │   └── svc-headless.yaml
│   │   ├── metrics
│   │   │   ├── deployment.yaml
│   │   │   ├── prometheusrule.yaml
│   │   │   ├── servicemonitor.yaml
│   │   │   └── service.yaml
│   │   ├── NOTES.txt
│   │   ├── secrets.yaml
│   │   ├── service.yaml
│   │   └── tls-secret.yaml
│   └── values.yaml
└── elasticsearch-19.1.4.tgz
```

## 部署

```bash
# helm install elasticsearch-cluster /usr/local/k8s/elasticsearch/elasticsearch-19.1.4.tgz -n iot
NAME: elasticsearch-cluster
LAST DEPLOYED: Fri Aug  5 11:49:51 2022
NAMESPACE: iot
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: elasticsearch
CHART VERSION: 19.1.4
APP VERSION: 8.3.3

-------------------------------------------------------------------------------
 WARNING

    Elasticsearch requires some changes in the kernel of the host machine to
    work as expected. If those values are not set in the underlying operating
    system, the ES containers fail to boot with ERROR messages.

    More information about these requirements can be found in the links below:

      https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html
      https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html

    This chart uses a privileged initContainer to change those settings in the Kernel
    by running: sysctl -w vm.max_map_count=262144 && sysctl -w fs.file-max=65536

** Please be patient while the chart is being deployed **

  Elasticsearch can be accessed within the cluster on port 9200 at elasticsearch-cluster.iot.svc.cluster.local

  To access from outside the cluster execute the following commands:

    kubectl port-forward --namespace iot svc/elasticsearch-cluster 9200:9200 &
    curl http://127.0.0.1:9200/
```

查看资源:

```bash
# kubectl get sts -n iot | grep elasticsearch
elasticsearch-cluster-coordinating   3/3     25m
elasticsearch-cluster-data           3/3     25m
elasticsearch-cluster-ingest         3/3     25m
elasticsearch-cluster-master         3/3     25m

# kubectl get pod -n iot -o wide | grep elasticsearch
elasticsearch-cluster-coordinating-0               1/1     Running     0               25m     10.244.1.149   centos-docker-164   <none>           <none>
elasticsearch-cluster-coordinating-1               1/1     Running     0               25m     10.244.2.134   centos-docker-165   <none>           <none>
elasticsearch-cluster-coordinating-2               1/1     Running     0               25m     10.244.0.14    centos-docker-163   <none>           <none>
elasticsearch-cluster-data-0                       1/1     Running     0               25m     10.244.2.135   centos-docker-165   <none>           <none>
elasticsearch-cluster-data-1                       1/1     Running     0               25m     10.244.1.152   centos-docker-164   <none>           <none>
elasticsearch-cluster-data-2                       1/1     Running     0               25m     10.244.0.16    centos-docker-163   <none>           <none>
elasticsearch-cluster-ingest-0                     1/1     Running     0               25m     10.244.2.133   centos-docker-165   <none>           <none>
elasticsearch-cluster-ingest-1                     1/1     Running     0               25m     10.244.1.151   centos-docker-164   <none>           <none>
elasticsearch-cluster-ingest-2                     1/1     Running     0               25m     10.244.0.13    centos-docker-163   <none>           <none>
elasticsearch-cluster-kibana-69655d859f-zgvlg      1/1     Running     0               25m     10.244.2.131   centos-docker-165   <none>           <none>
elasticsearch-cluster-master-0                     1/1     Running     0               25m     10.244.2.132   centos-docker-165   <none>           <none>
elasticsearch-cluster-master-1                     1/1     Running     0               25m     10.244.1.150   centos-docker-164   <none>           <none>
elasticsearch-cluster-master-2                     1/1     Running     0               25m     10.244.0.15    centos-docker-163   <none>           <none>

# kubectl get pvc,pv -n iot | grep elasticsearch
persistentvolumeclaim/data-elasticsearch-cluster-data-0       Bound    pvc-81e29ecd-734b-445a-a3e6-74310f4c01f2   8Gi        RWO            nfs-client     4h13m
persistentvolumeclaim/data-elasticsearch-cluster-data-1       Bound    pvc-62b16510-1879-4e5b-863f-31dfd34ad979   8Gi        RWO            nfs-client     4h13m
persistentvolumeclaim/data-elasticsearch-cluster-data-2       Bound    pvc-2d919ca5-007f-43b0-8bba-623ac6fc695b   8Gi        RWO            nfs-client     25m
persistentvolumeclaim/data-elasticsearch-cluster-master-0     Bound    pvc-98fb9a23-de17-44d8-8a6b-3cbbf5c888a6   8Gi        RWO            nfs-client     4h13m
persistentvolumeclaim/data-elasticsearch-cluster-master-1     Bound    pvc-b1f5e3c2-0351-4ef4-b736-757445cc1ba1   8Gi        RWO            nfs-client     4h13m
persistentvolumeclaim/data-elasticsearch-cluster-master-2     Bound    pvc-2430ada6-d3d1-4ca7-ad1e-7f004d7dc695   8Gi        RWO            nfs-client     25m
persistentvolumeclaim/elasticsearch-cluster-kibana            Bound    pvc-943c7d61-698c-4210-8cd0-a234a5b837f4   10Gi       RWO            nfs-client     25m
persistentvolume/pvc-2430ada6-d3d1-4ca7-ad1e-7f004d7dc695   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-master-2     nfs-client              25m
persistentvolume/pvc-2d919ca5-007f-43b0-8bba-623ac6fc695b   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-data-2       nfs-client              25m
persistentvolume/pvc-62b16510-1879-4e5b-863f-31dfd34ad979   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-data-1       nfs-client              4h13m
persistentvolume/pvc-81e29ecd-734b-445a-a3e6-74310f4c01f2   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-data-0       nfs-client              4h13m
persistentvolume/pvc-943c7d61-698c-4210-8cd0-a234a5b837f4   10Gi       RWO            Delete           Bound    iot/elasticsearch-cluster-kibana            nfs-client              25m
persistentvolume/pvc-98fb9a23-de17-44d8-8a6b-3cbbf5c888a6   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-master-0     nfs-client              4h13m
persistentvolume/pvc-b1f5e3c2-0351-4ef4-b736-757445cc1ba1   8Gi        RWO            Delete           Bound    iot/data-elasticsearch-cluster-master-1     nfs-client              4h13m

# kubectl get svc -n iot | grep elasticsearch
elasticsearch-cluster                   ClusterIP   10.109.140.237   <none>        9200/TCP,9300/TCP                                26m
elasticsearch-cluster-coordinating-hl   ClusterIP   None             <none>        9200/TCP,9300/TCP                                26m
elasticsearch-cluster-data-hl           ClusterIP   None             <none>        9200/TCP,9300/TCP                                26m
elasticsearch-cluster-ingest-hl         ClusterIP   None             <none>        9200/TCP,9300/TCP                                26m
elasticsearch-cluster-kibana            ClusterIP   10.100.91.85     <none>        5601/TCP                                         26m
elasticsearch-cluster-master-hl         ClusterIP   None             <none>        9200/TCP,9300/TCP                                26m
```

## 内部访问 elasticsearch 集群

在 elasticsearch-cluster-coordinating-0 的 pod 里新建 ```foo``` 索引:

```bash
# kubectl exec -it elasticsearch-cluster-coordinating-0 -n iot -- /bin/sh

$ curl -k -u elastic:root https://localhost:9200/_cat/indices?v
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size

$ curl -k -u elastic:root -XPUT https://localhost:9200/foo/?pretty
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "foo"
}

$ curl -k -u elastic:root https://localhost:9200/_cat/indices?v
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   foo   guioAO1vSt2u7onjZ3t6FA   1   1          0            0       450b           225b
```

在 elasticsearch-cluster-coordinating-1 的 pod 里查看索引同步情况:

```bash
# kubectl exec -it elasticsearch-cluster-coordinating-1 -n iot -- /bin/sh

$ curl -k -u elastic:root https://localhost:9200/_cat/indices?v
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   foo   guioAO1vSt2u7onjZ3t6FA   1   1          0            0       450b           225b
```

在 elasticsearch-cluster-coordinating-2 的 pod 里查看索引同步情况:

```bash
# kubectl exec -it elasticsearch-cluster-coordinating-2 -n iot -- /bin/sh

$ curl -k -u elastic:root https://localhost:9200/_cat/indices?v
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   foo   guioAO1vSt2u7onjZ3t6FA   1   1          0            0       450b           225b
```

## 外部访问 elasticsearch 集群

创建 NodePort 类型的 Service:

```bash
# vim /usr/local/k8s/elasticsearch/service.yaml
```

```yml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-cluster-service
  namespace: iot
spec:
  selector:
    app.kubernetes.io/component: coordinating-only
    app.kubernetes.io/instance: elasticsearch-cluster
    app.kubernetes.io/name: elasticsearch
  ports:
    - name: tcp-rest-api
      protocol: TCP
      port: 9200
      targetPort: rest-api
      nodePort: 30200
    - name: tcp-transport
      protocol: TCP
      port: 9300
      targetPort: transport
      nodePort: 30300
  type: NodePort
```

```bash
# kubectl apply -f /usr/local/k8s/elasticsearch/service.yaml

# kubectl get svc elasticsearch-cluster-service -n iot
NAME                            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
elasticsearch-cluster-service   NodePort   10.106.47.139   <none>        9200:30200/TCP,9300:30300/TCP   2m21s
```

外部服务器连接 elasticsearch 集群:

```bash
# curl -k -u elastic:root -XPUT https://192.168.5.163:30200/bar/?pretty
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "bar"
}

# curl -k -u elastic:root -XGET https://192.168.5.163:30200/_cat/indices?v
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   bar   juntQJk-ToWD8WiY-7xsUQ   1   1          0            0       450b           225b
green  open   foo   guioAO1vSt2u7onjZ3t6FA   1   1          0            0       450b           225b

# curl -k -u elastic:root -XDELETE https://192.168.5.163:30200/foo?pretty
{
  "acknowledged" : true
}

# curl -k -u elastic:root -XGET https://192.168.5.163:30200/_cat/indices?v
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   bar   juntQJk-ToWD8WiY-7xsUQ   1   1          0            0       450b           225b
```

## 外部访问 kibana

创建 NodePort 类型的 Service:

```bash
# vim /usr/local/k8s/elasticsearch/kibana-service.yaml
```

```yml
apiVersion: v1
kind: Service
metadata:
  name: kibana-service
  namespace: iot
spec:
  selector:
    app.kubernetes.io/instance: elasticsearch-cluster
    app.kubernetes.io/name: kibana
  ports:
    - name: http
      protocol: TCP
      port: 5601
      targetPort: http
      nodePort: 30601
  type: NodePort
```

```bash
# kubectl apply -f /usr/local/k8s/elasticsearch/kibana-service.yaml

# kubectl get svc kibana-service -n iot
# kubectl get svc kibana-service -n iot
NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kibana-service   NodePort   10.110.179.33   <none>        5601:30601/TCP   2m21s
```

在外部服务器的浏览器地址栏里输入以下链接:

- 未开启 X-Pack:

    ```
    http://192.168.5.163:30601
    ```

- 开启 X-Pack，用户名/密码: ```elastic/root```:

    ```
    https://192.168.5.163:30601
    ```

## Elasticsearch 角色

1. master

    ```
    master节点具备主节点的选举权，有资格成为主节点，主节点控制整个集群的元数据(metadata)，比如索引的新增、删除、分片分配等；
    该节点不和应用创建连接，master节点不占用IO和CPU，内存使用量一般。

    角色配置方式：node.master: true
    ```

2. data

    ```
    该节点和应用创建连接、接收索引请求，会存储分配在该node上的shard的数据并负责这些shard的写入、查询等，ES集群的性能取决于该节点的个数（每个节点最优配置的情况下），data节点会占用大量的CPU、io和内存；
    data节点的分片执行查询语句、获得查询结果后将结果反馈给Coordinating，此过程较消耗硬件资源。

    角色配置方式：node.data: true
    ```

3. coordinating

    ```
    该节点和检索应用创建连接、接受检索请求，但其本身不负责存储数据，可当负责均衡节点，Coordinating节点不占用io、cpu和内存；
    Coordinating节点接受搜索请求后将请求转发到与查询条件相关的多个data节点的分片上，然后多个data节点的分片执行查询语句或者查询结果再返回给Coordinating节点，Coordinating来把各个data节点的返回结果进行整合、排序等一系列操作后再将最终结果返回给用户请求。

    角色配置方式：node.master: false，node.data: false
    ```

4. ingest

    ```
    可以在任何节点上启用 ingest，甚至使用专门的 ingest nodes；
    Ingest node 专门对索引的文档做预处理，发生在对真实文档建立索引之前。在建立索引对文档预处理之前，先定义一个管道（pipeline），管道里指定了一系列的处理器。每个处理器能够把文档按照某种特定的方式转换。比如在管道里定义一个从某个文档中移除字段的处理器，紧接着一个重命名字段的处理器。集群的状态也会被存储到配置的管道内。定义一个管道，简单的在索引或者bulk request(一种批量请求方法)操作上定义 pipeline 参数,这样 ingest node 就会知道哪个管道在使用。

    角色配置方式：node.ingest: true
    ```
